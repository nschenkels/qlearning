<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2020-10-30 Fri 11:31 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>Notes on Q-learning and deep Q-learning</title>
<meta name="generator" content="Org mode" />
<meta name="author" content="N. De Neuter, N. Schenkels" />
<style type="text/css">
 <!--/*--><![CDATA[/*><!--*/
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #ccc;
    box-shadow: 3px 3px 3px #eee;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: visible;
    padding-top: 1.2em;
  }
  pre.src:before {
    display: none;
    position: absolute;
    background-color: white;
    top: -10px;
    right: 10px;
    padding: 3px;
    border: 1px solid black;
  }
  pre.src:hover:before { display: inline;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { width: 90%; }
  /*]]>*/-->
</style>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/htmlize.css"/>
<link rel="stylesheet" type="text/css" href="https://fniessen.github.io/org-html-themes/styles/readtheorg/css/readtheorg.css"/>
<script src="https://ajax.googleapis.com/ajax/libs/jquery/2.1.3/jquery.min.js"></script>
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.4/js/bootstrap.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/lib/js/jquery.stickytableheaders.min.js"></script>
<script type="text/javascript" src="https://fniessen.github.io/org-html-themes/styles/readtheorg/js/readtheorg.js"></script>
<div style="display: none"> \(
\newcommand{\mbbR}{\mathbb{R}}
\DeclareMathOperator*{\argmax}{argmax}
\)</div>
<script type="text/javascript">
/*
@licstart  The following is the entire license notice for the
JavaScript code in this tag.

Copyright (C) 2012-2020 Free Software Foundation, Inc.

The JavaScript code in this tag is free software: you can
redistribute it and/or modify it under the terms of the GNU
General Public License (GNU GPL) as published by the Free Software
Foundation, either version 3 of the License, or (at your option)
any later version.  The code is distributed WITHOUT ANY WARRANTY;
without even the implied warranty of MERCHANTABILITY or FITNESS
FOR A PARTICULAR PURPOSE.  See the GNU GPL for more details.

As additional permission under GNU GPL version 3 section 7, you
may distribute non-source (e.g., minimized or compacted) forms of
that code without the copy of the GNU GPL normally required by
section 4, provided you include this license notice and a URL
through which recipients can access the Corresponding Source.


@licend  The above is the entire license notice
for the JavaScript code in this tag.
*/
<!--/*--><![CDATA[/*><!--*/
 function CodeHighlightOn(elem, id)
 {
   var target = document.getElementById(id);
   if(null != target) {
     elem.cacheClassElem = elem.className;
     elem.cacheClassTarget = target.className;
     target.className = "code-highlighted";
     elem.className   = "code-highlighted";
   }
 }
 function CodeHighlightOff(elem, id)
 {
   var target = document.getElementById(id);
   if(elem.cacheClassElem)
     elem.className = elem.cacheClassElem;
   if(elem.cacheClassTarget)
     target.className = elem.cacheClassTarget;
 }
/*]]>*///-->
</script>
<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        displayAlign: "center",
        displayIndent: "0em",

        "HTML-CSS": { scale: 100,
                        linebreaks: { automatic: "false" },
                        webFont: "TeX"
                       },
        SVG: {scale: 100,
              linebreaks: { automatic: "false" },
              font: "TeX"},
        NativeMML: {scale: 100},
        TeX: { equationNumbers: {autoNumber: "AMS"},
               MultLineWidth: "85%",
               TagSide: "right",
               TagIndent: ".8em"
             }
});
</script>
<script type="text/javascript"
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS_HTML"></script>
</head>
<body>
<div id="content">
<h1 class="title">Notes on Q-learning and deep Q-learning</h1>
<div id="table-of-contents">
<h2>Table of Contents</h2>
<div id="text-table-of-contents">
<ul>
<li><a href="#org48945da">Introduction</a></li>
<li><a href="#org62eeeb7">Q-learning</a>
<ul>
<li><a href="#orgd06f1c0">Q-table</a></li>
<li><a href="#org161fa4b">The learning process</a></li>
<li><a href="#orga099aee">Notes on target, reward, and stochastic version</a></li>
</ul>
</li>
<li><a href="#org7d53276">Deep Q-learning</a>
<ul>
<li><a href="#org8785fac">The learning process</a></li>
<li><a href="#org58d1aee">Updating the neural network(s)</a></li>
</ul>
</li>
<li><a href="#org6207c1b">Representing a game environment</a></li>
<li><a href="#org2ae2bb4">References &amp; further reading</a></li>
</ul>
</div>
</div>

<div id="outline-container-org48945da" class="outline-2">
<h2 id="org48945da">Introduction</h2>
<div class="outline-text-2" id="text-org48945da">
<p>
The goal is the learn the Q-learning agent to play a game. A game consists of:
</p>
<ul class="org-ul">
<li>A set of states \(S\).</li>
<li>A set of actions \(A\).</li>
<li>A reward function \(r: S\times A\rightarrow \mbbR\) that links a reward to
every state-action pair \((s, a)\).</li>
</ul>
</div>
</div>

<div id="outline-container-org62eeeb7" class="outline-2">
<h2 id="org62eeeb7">Q-learning</h2>
<div class="outline-text-2" id="text-org62eeeb7">
</div>
<div id="outline-container-orgd06f1c0" class="outline-3">
<h3 id="orgd06f1c0">Q-table</h3>
<div class="outline-text-3" id="text-orgd06f1c0">
<p>
We will assume for now that \(S\) and \(A\) are finite, with \(m\) and \(n\) elements
respectively. In the case that a game has infinite states or actions - or when
the number of elements is to high for practical consideration - a discrete
representation can be used to map every state-action pair to an element of
\(S\times A\).
</p>

<p>
With each state-action \((s, a)\) we can associate a quality value (Q-value) that
represents how good it is to take action \(a\) in state \(s\). Here, "good" should
be interpreted in the sense of getting closer to achieving the goal of the game.
We can think of this as a function
\[
    Q: S\times A\longrightarrow \mbbR: (s, a)\mapsto q,
\]
and since \(S\) and \(A\) are finite, represent this function in a so-called
Q-table:
</p>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />

<col  class="org-left" />
</colgroup>
<thead>
<tr>
<th scope="col" class="org-left">&#xa0;</th>
<th scope="col" class="org-left">a<sub>1</sub></th>
<th scope="col" class="org-left">a<sub>2</sub></th>
<th scope="col" class="org-left">&#x2026;</th>
<th scope="col" class="org-left">a<sub>n</sub></th>
</tr>
</thead>
<tbody>
<tr>
<td class="org-left">s<sub>1</sub></td>
<td class="org-left">q<sub>11</sub></td>
<td class="org-left">q<sub>12</sub></td>
<td class="org-left">&#x2026;</td>
<td class="org-left">q<sub>1n</sub></td>
</tr>

<tr>
<td class="org-left">s<sub>2</sub></td>
<td class="org-left">q<sub>21</sub></td>
<td class="org-left">q<sub>22</sub></td>
<td class="org-left">&#x2026;</td>
<td class="org-left">q<sub>2n</sub></td>
</tr>

<tr>
<td class="org-left">\(\vdots\)</td>
<td class="org-left">\(\vdots\)</td>
<td class="org-left">\(\vdots\)</td>
<td class="org-left">\(\ddots\)</td>
<td class="org-left">\(\vdots\)</td>
</tr>

<tr>
<td class="org-left">s<sub>m</sub></td>
<td class="org-left">q<sub>m1</sub></td>
<td class="org-left">q<sub>m2</sub></td>
<td class="org-left">&#x2026;</td>
<td class="org-left">q<sub>mn</sub></td>
</tr>
</tbody>
</table>
</div>
</div>

<div id="outline-container-org161fa4b" class="outline-3">
<h3 id="org161fa4b">The learning process</h3>
<div class="outline-text-3" id="text-org161fa4b">
<p>
The idea behind Q-learning is to let the agent learn/discover the
Q-values for each state-action pair \((s_i, a_j)\) automatically. In order to do
this, we start from an initial (random) Q-table and try to improve the Q-values
by playing the game over and over and looking at the rewards of all the actions
that were taken. A rough sketch would be:
</p>
<ul class="org-ul">
<li>Start from an initial state.</li>
<li>Perform an action and advance the game to a new state.</li>
<li>Update the Q-table based on the result of the action (see further).</li>
<li>If the game terminates - for whatever reason: win, loss, time-out, &#x2026; -
start again from a new initial state.</li>
<li>Keep doing this until the agent has learned to play the game successfully.</li>
</ul>
<p>
The longer this is done, the more accurate the values in the (approximate)
Q-table should become to their true (unknown) values and the better the agent
should become at playing the game.
</p>

<p>
If, during the Q-learing algorithm, we are in state \(s_t\) and take action \(a_t\),
then the corresponding Q-value is updated as follows:
</p>
\begin{equation}
\label{org2615645}
    Q(s_t, a_t) \longleftarrow Q(s_t, a_t) + \alpha\left(r_t +
        \gamma\argmax_aQ(s_{t + 1}, a) - Q(s_t, a_t)\right)
\end{equation}
<p>
This formula has the following parameters:
</p>
<ul class="org-ul">
<li>\(r_t = r(s_t, a_t)\) is the reward from taking action \(a_t\) in state \(s_t\). A
higher reward leads to a higher updated Q-value.</li>
<li>\(\alpha\) is the learning rate. Higher values lead to higher updated Q-value.</li>
<li>\(s_{t + 1}\) is the state in which we arrive after taking action \(a_t\) in state
\(s_t\). The term \(\gamma\argmax_aQ(s_{t + 1}, a)\) adds a contribution based on
the Q-value of the actions we can take in the next state. Here, \(\gamma\) is a
discount factor that puts more or less weight on these future actions.</li>
</ul>

<p>
During the Q-learning algorithm there are two ways of choosing which action to
take. The first is to simply take the action with the highest Q-value. In order
to let the agent try different strategies, however, the second option is to take
a random action. In order to not keep taking random actions the chance of taking
a random action should decrease as the algorithm performs more and more steps.
</p>
</div>
</div>

<div id="outline-container-orga099aee" class="outline-3">
<h3 id="orga099aee">Notes on target, reward, and stochastic version</h3>
</div>
</div>

<div id="outline-container-org7d53276" class="outline-2">
<h2 id="org7d53276">Deep Q-learning</h2>
<div class="outline-text-2" id="text-org7d53276">
<p>
Q-learning works well for simple games where we can represent all state-action
pairs in a table (or at least a good finite representation of them). For many
games, however, this is simply impossible: the number of states and or actions
is just to high, or we run into memory restrictions, etc. In these cases deep
Q-learning is better suited. The idea is to train a neural network to predict
the q-values. More precisely, given a game state \(s\), the neural network would
return a list of Q-values corresponding to the row of \(s\) in the Q-table.
As stated before, this approach should work better for more complex games, e.g.,
many actions, (infinitely) many states, high memory requirements, &#x2026;
</p>
</div>

<div id="outline-container-org8785fac" class="outline-3">
<h3 id="org8785fac">The learning process</h3>
<div class="outline-text-3" id="text-org8785fac">
<p>
The learning process for deep Q-learning is essentially the same as for
Q-learning. The difference lies in how we get our Q-values - from a neural
network instead of from a table - and how we update these neural network -
instead of updating the Q-table. Two notable differences:
</p>
<ul class="org-ul">
<li>If the action taken is not random, the state is given to the neural network,
a fit is performed, and the output (= Q-values) is used to decide which
action is taken.</li>
<li>Based on the result of the action the weights of the neural network are
updated (see further).</li>
</ul>
</div>
</div>

<div id="outline-container-org58d1aee" class="outline-3">
<h3 id="org58d1aee">Updating the neural network(s)</h3>
<div class="outline-text-3" id="text-org58d1aee">
<p>
After the action is taken and the new game state and reward are calculated,
we want to update the neural network. In Q-learning, we simply used Equation
\eqref{org2615645} to update a single value at a time. Since the neural network
never gives back the Q-value for one action, but for all the actions given a
state, we will now calculate the updates weights for all actions given a state
\(s\) using Equation \eqref{org2615645} and then fit the neural network to these
updated values.
</p>
</div>
</div>
</div>

<div id="outline-container-org6207c1b" class="outline-2">
<h2 id="org6207c1b">Representing a game environment</h2>
<div class="outline-text-2" id="text-org6207c1b">
<p>
A game environment is represented by a python class with the following
</p>
<ul class="org-ul">
<li>state: a structure that represents the state of the game.</li>
<li>reward: the reward of the current state.</li>
<li>done: a boolean value indicating if the game is in a terminal state.</li>
<li>info: a dictionary with extra game information.</li>
<li>actions: a dictionary of possible actions with integer keys.</li>
</ul>
<p>
The following methods are also available:
</p>
<ul class="org-ul">
<li>reset: resets the state to an initial game state and returns this
state.</li>
<li>step: takes an action and performs this action, updating the game state.
This method returns self.state, self.reward, self.done, and self.info.</li>
<li>render: visualizes the current game state.</li>
</ul>
</div>
</div>

<div id="outline-container-org2ae2bb4" class="outline-2">
<h2 id="org2ae2bb4">References &amp; further reading</h2>
<div class="outline-text-2" id="text-org2ae2bb4">
<ul class="org-ul">
<li>Sentdex's introduction to <a href="https://pythonprogramming.net/introduction-deep-learning-python-tensorflow-keras/">deep learning</a></li>
<li>Sentdex's  tutorial on <a href="https://pythonprogramming.net/q-learning-reinforcement-learning-python-tutorial/">Q-learning and deep Q-learning</a>.</li>
<li>Q-learning on <a href="https://en.wikipedia.org/wiki/Q-learning">Wikipedia</a>.</li>
<li><a href="https://www.analyticsvidhya.com/blog/2019/04/introduction-deep-q-learning-python/">weblink 1</a></li>
<li><a href="https://www.mlq.ai/deep-reinforcement-learning-q-learning/">weblink 2</a></li>
</ul>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: N. De Neuter, N. Schenkels</p>
<p class="date">Created: 2020-10-30 Fri 11:31</p>
<p class="validation"><a href="http://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
